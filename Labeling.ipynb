{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYlTxDNVmrcl",
        "outputId": "be3e51dc-6a47-4cee-8c30-8865817b58ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zZj1FBqmj0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57d6156-c59b-41f5-d64e-5c1ed0a2c77c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading sentiment analysis model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Dataset loaded with 237019 rows.\n",
            "Analyzing sentiment for English sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 11/237019 [00:00<3:10:42, 20.71it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "100%|██████████| 237019/237019 [35:36<00:00, 110.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Sentiment analysis complete! Labeled dataset saved to: /content/drive/MyDrive/Final_Labeled_Dataset.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "print(\"Loading sentiment analysis model...\")\n",
        "sentiment_pipeline = pipeline(\"text-classification\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
        "\n",
        "# Function to get sentiment label\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        if pd.isna(text) or str(text).strip() == \"\":\n",
        "            return \"N\"  # Neutral as default for empty text\n",
        "\n",
        "        result = sentiment_pipeline(text[:512])  # Truncate to model's max length\n",
        "        label = result[0]['label']\n",
        "\n",
        "        # Convert to your desired format (P/N)\n",
        "        if label == 'POS':\n",
        "            return \"P\"\n",
        "        elif label == 'NEG':\n",
        "            return \"N\"\n",
        "        else:  # NEU\n",
        "            return \"M\"  # Or you might want to use another code for neutral\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {text}. Error: {str(e)}\")\n",
        "        return \"E\"\n",
        "\n",
        "# Load your dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset_path = r\"/content/drive/MyDrive/Dataset.csv\"\n",
        "df = pd.read_csv(dataset_path, encoding='utf-8-sig')\n",
        "print(f\"Dataset loaded with {len(df)} rows.\")\n",
        "# Process English sentences and add sentiment labels\n",
        "print(\"Analyzing sentiment for English sentences...\")\n",
        "tqdm.pandas()  # Enable progress bar for pandas apply\n",
        "df['Sentiment(P/N)'] = df['English_Sentence_text'].progress_apply(get_sentiment)\n",
        "\n",
        "# Save the labeled dataset\n",
        "output_path = r\"/content/drive/MyDrive/Final_Labeled_Dataset.csv\"\n",
        "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n✅ Sentiment analysis complete! Labeled dataset saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load dataset\n",
        "input_path = r\"/content/drive/MyDrive/Final_Labeled_Dataset.csv\"\n",
        "df = pd.read_csv(input_path, encoding='utf-8-sig')\n",
        "print(\"Original dataset shape:\", df.shape)\n",
        "\n",
        "# 1. Remove neutral sentiment\n",
        "df_filtered = df[df['Sentiment(P/N)'] != 'M'].copy()\n",
        "print(\"After removing neutral:\", df_filtered.shape)\n",
        "\n",
        "# 2. Remove short sentences (<5 words)\n",
        "def count_urdu_words(sentence):\n",
        "    if pd.isna(sentence):\n",
        "        return 0\n",
        "    words = re.sub(r'[۔،؛؟!\\s]+', ' ', str(sentence)).strip().split()\n",
        "    return len(words)\n",
        "\n",
        "df_filtered['word_count'] = df_filtered['Sentence_text'].apply(count_urdu_words)\n",
        "df_filtered = df_filtered[df_filtered['word_count'] >= 5].drop(columns=['word_count'])\n",
        "print(\"After removing short sentences:\", df_filtered.shape)\n",
        "\n",
        "# 3. Balance sentiments (max 20k each)\n",
        "MAX_SAMPLES = 20000\n",
        "\n",
        "# Split by sentiment\n",
        "pos = df_filtered[df_filtered['Sentiment(P/N)'] == 'P']\n",
        "neg = df_filtered[df_filtered['Sentiment(P/N)'] == 'N']\n",
        "\n",
        "# Limit to 20k each (or available if less)\n",
        "pos_limited = pos.head(min(len(pos), MAX_SAMPLES))\n",
        "neg_limited = neg.head(min(len(neg), MAX_SAMPLES))\n",
        "\n",
        "# Get extra negatives beyond 20k\n",
        "neg_extra = neg.iloc[MAX_SAMPLES:] if len(neg) > MAX_SAMPLES else pd.DataFrame()\n",
        "\n",
        "# Combine balanced dataset\n",
        "df_balanced = pd.concat([pos_limited, neg_limited]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# 4. Save results\n",
        "# Balanced dataset\n",
        "balanced_path = r\"/content/drive/MyDrive/Ultra_Final_Labeled_Dataset.csv\"\n",
        "df_balanced.to_csv(balanced_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# Extra negatives (if any)\n",
        "if len(neg_extra) > 0:\n",
        "    extra_path = r\"/content/drive/MyDrive/Negative_Final_Labeled_Dataset.csv\"\n",
        "    neg_extra.to_csv(extra_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# 5. Print statistics\n",
        "print(\"\\nFinal Counts:\")\n",
        "print(f\"Positive: {len(pos_limited)}\")\n",
        "print(f\"Negative: {len(neg_limited)}\")\n",
        "if len(neg_extra) > 0:\n",
        "    print(f\"Extra negatives saved: {len(neg_extra)}\")\n",
        "\n",
        "print(f\"\\n✅ Balanced dataset saved to: {balanced_path}\")\n",
        "if len(neg_extra) > 0:\n",
        "    print(f\"✅ Extra negatives saved to: {extra_path}\")\n",
        "\n",
        "print(\"\\nSample of balanced data:\")\n",
        "print(df_balanced[['Sentence_text', 'Sentiment(P/N)']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEAllX4nzrA3",
        "outputId": "0743fb5c-b997-4ce1-a315-7e4dcd7ccde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape: (38258, 8)\n",
            "After removing neutral: (38258, 8)\n",
            "After removing short sentences: (38258, 8)\n",
            "\n",
            "Final Counts:\n",
            "Positive: 18258\n",
            "Negative: 20000\n",
            "\n",
            "✅ Balanced dataset saved to: /content/drive/MyDrive/Ultra_Final_Labeled_Dataset.csv\n",
            "\n",
            "Sample of balanced data:\n",
            "                             Sentence_text Sentiment(P/N)\n",
            "0       لیکن کم از کم وہ ہم سے مل سکتا ہے۔              P\n",
            "1          تو میں وکیل کے پاس نہیں جاسکتا۔              N\n",
            "2  آپ کا خوبصورت نمو آپ سے کیا کہہ رہا ہے؟              P\n",
            "3   آپ نے جو کچھ بھی کیا ، یہ بہت غلط تھا۔              N\n",
            "4   میں نے سوچا کہ آپ کسی پریشانی میں ہیں۔              N\n"
          ]
        }
      ]
    }
  ]
}